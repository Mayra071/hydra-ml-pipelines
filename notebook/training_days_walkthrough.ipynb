{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Pipelines Walkthrough (Day 1–5)\n",
        "\n",
        "This notebook documents and demonstrates the modular ML training pipeline using Hydra and MLflow. It aligns with the day-wise plan:\n",
        "\n",
        "- Day 1: Define pipeline structure (load → preprocess → train → eval) and configs\n",
        "- Day 2: Implement Logistic Regression & Random Forest and log metrics\n",
        "- Day 3: Add cross-validation and hyperparameter search (GridSearchCV)\n",
        "- Day 4: Add Gradient Boosting (optionally XGBoost)\n",
        "- Day 5: Demo run and reproducibility notes\n",
        "\n",
        "Prerequisites:\n",
        "- Ensure `requirements.txt` is installed in your Python environment.\n",
        "- Run from the project root so paths resolve correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Day 1: Pipeline structure and configs\n",
        "\n",
        "- Source layout:\n",
        "  - `main.py`: Hydra entrypoint; orchestrates ingestion → split → preprocess → train → log\n",
        "  - `src/Training_pipeline/components`: ingestion, transformation, model training\n",
        "  - `src/Training_pipeline/pipeline`: preprocessing & training pipeline wrappers\n",
        "  - `configs/`: Hydra config groups: `dataset`, `model`, `training`, `preprocessing`\n",
        "- Key configs:\n",
        "  - `configs/config.yaml` sets defaults and MLflow tracking\n",
        "  - `configs/dataset/cal_housing.yaml` and `cal_housing_classification.yaml`\n",
        "  - `configs/model/*.yaml`: random_forest, logistic_regression, gradient_boosting (xgboost optional)\n",
        "  - `configs/training/base.yaml`: CV and search settings\n",
        "  - `configs/preprocessing/standard.yaml`: imputer/scaler/encoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kaggle setup: install dependencies and set dataset path\n",
        "import os, sys, subprocess\n",
        "\n",
        "# Install requirements (safe to re-run)\n",
        "%pip install -q -r ../requirements.txt\n",
        "\n",
        "# Define the dataset CSV path for Kaggle; update this if your dataset path differs\n",
        "# Example for a Kaggle dataset named \"california-housing-prices\":\n",
        "# /kaggle/input/california-housing-prices/housing.csv\n",
        "DATASET_PATH = os.environ.get(\n",
        "    \"DATASET_PATH\",\n",
        "    \"/kaggle/input/california-housing-prices/housing.csv\"\n",
        ")\n",
        "print(\"Using DATASET_PATH=\", DATASET_PATH)\n",
        "\n",
        "# Helper to build a train command with dataset path override\n",
        "\n",
        "def train_cmd(dataset_group: str, model: str, extra_overrides: str = \"\") -> str:\n",
        "    overrides = f\"dataset={dataset_group} model={model} dataset.paths.dataset=\\\"{DATASET_PATH}\\\"\"\n",
        "    if extra_overrides:\n",
        "        overrides += \" \" + extra_overrides\n",
        "    return f\"python .\\\\train.py {overrides}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, json, subprocess, textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path(os.getcwd())\n",
        "print(\"Project root:\", PROJECT_ROOT)\n",
        "\n",
        "# Utility: run a command and stream output\n",
        "\n",
        "def run(cmd: str):\n",
        "    print(f\"\\n$ {cmd}\")\n",
        "    proc = subprocess.Popen(cmd, shell=True)\n",
        "    proc.wait()\n",
        "    if proc.returncode != 0:\n",
        "        raise RuntimeError(f\"Command failed with code {proc.returncode}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Day 2: Baselines (Logistic Regression, Random Forest)\n",
        "\n",
        "We'll run two quick training jobs: one for regression, one for classification with quantile labeling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Kaggle-friendly runs (use DATASET_PATH override)\n",
        "These cells use `train_cmd()` so the dataset path under `/kaggle/input/...` is passed to Hydra.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regression baseline (Random Forest)\n",
        "run(\"python .\\\\train.py dataset=cal_housing model=random_forest\")\n",
        "\n",
        "# Classification baseline (Logistic Regression)\n",
        "run(\"python .\\\\train.py dataset=cal_housing_classification model=logistic_regression\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Day 3: Hyperparameter Search + Cross-Validation\n",
        "\n",
        "`GridSearchCV` and CV folds are controlled by `configs/training/base.yaml`. We can adjust `search.cv`, `runtime.n_jobs`, and scoring keys.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: run with more folds and verbose logging\n",
        "run(\"python .\\\\train.py dataset=cal_housing model=random_forest training.search.cv=3 training.runtime.verbose=2\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Day 4: Add Gradient Boosting (and optional XGBoost)\n",
        "\n",
        "- To compare models, simply change the `model` group override.\n",
        "- If you re-enable XGBoost in the trainer and `configs/model/xgboost.yaml` exists, you can run it similarly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Gradient Boosting on regression\n",
        "run(\"python .\\\\train.py dataset=cal_housing model=gradient_boosting\")\n",
        "\n",
        "# Compare Gradient Boosting on classification\n",
        "run(\"python .\\\\train.py dataset=cal_housing_classification model=gradient_boosting\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Day 5: Demo + Reproducibility\n",
        "\n",
        "- Demo command (regression): `python .\\train.py dataset=cal_housing model=random_forest`\n",
        "- Demo command (classification): `python .\\train.py dataset=cal_housing_classification model=random_forest`\n",
        "- MLflow UI: `mlflow ui --backend-store-uri mlruns` (or your absolute mlruns path)\n",
        "\n",
        "Artifacts:\n",
        "- Data: `artifacts/`\n",
        "- Models: `models/`\n",
        "- Preprocessor: `artifacts/preprocessor.pkl`\n",
        "- Metrics: `artifacts/metrics.json`\n",
        "\n",
        "The run logs contain the composed config so you can reproduce a run by reusing the logged parameters and the config overrides shown here.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
